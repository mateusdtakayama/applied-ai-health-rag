{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Configuration loaded\n",
            "✓ Project root added to path: /home/mateusdelai/Desktop/applied-ai-health-rag\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Add project root to path for imports\n",
        "notebook_dir = Path.cwd()\n",
        "project_root = notebook_dir.parent if notebook_dir.name == \"notebooks\" else notebook_dir\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Configuration\n",
        "EVAL_DIR = Path(\"../data/eval\")\n",
        "PERSIST_DIRECTORY = \"../chroma_db\"\n",
        "COLLECTION_NAME = \"health_education_chunks\"\n",
        "MODEL_NAME = \"openai:gpt-4o-mini\"\n",
        "TEMPERATURE = 0.0\n",
        "K_RETRIEVAL = 3\n",
        "\n",
        "print(\"✓ Configuration loaded\")\n",
        "print(f\"✓ Project root added to path: {project_root}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4 evaluation files:\n",
            "  - qa_ground_truth_diabetes_overview_v2.json\n",
            "  - qa_ground_truth_diabetes_treatment.json\n",
            "  - qa_ground_truth_hypertension_overview.json\n",
            "  - qa_ground_truth_hypertension_treatment.json\n",
            "  Loaded 12 questions from qa_ground_truth_diabetes_overview_v2.json\n",
            "  Loaded 9 questions from qa_ground_truth_diabetes_treatment.json\n",
            "  Loaded 6 questions from qa_ground_truth_hypertension_overview.json\n",
            "  Loaded 9 questions from qa_ground_truth_hypertension_treatment.json\n",
            "\n",
            "✓ Total of 36 questions loaded\n"
          ]
        }
      ],
      "source": [
        "# Load all evaluation files\n",
        "eval_files = sorted(EVAL_DIR.glob(\"*.json\"))\n",
        "print(f\"Found {len(eval_files)} evaluation files:\")\n",
        "for f in eval_files:\n",
        "    print(f\"  - {f.name}\")\n",
        "\n",
        "# Load all evaluation data\n",
        "all_eval_data = []\n",
        "for eval_file in eval_files:\n",
        "    with open(eval_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "        # Add file name for reference\n",
        "        for item in data:\n",
        "            item[\"eval_file\"] = eval_file.name\n",
        "        all_eval_data.extend(data)\n",
        "    print(f\"  Loaded {len(data)} questions from {eval_file.name}\")\n",
        "\n",
        "print(f\"\\n✓ Total of {len(all_eval_data)} questions loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Vector store loaded! Contains 131 documents\n"
          ]
        }
      ],
      "source": [
        "# Initialize ChromaDB vector store\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vector_store = Chroma(\n",
        "    persist_directory=PERSIST_DIRECTORY,\n",
        "    embedding_function=embeddings,\n",
        "    collection_name=COLLECTION_NAME,\n",
        ")\n",
        "\n",
        "doc_count = vector_store._collection.count()\n",
        "print(f\"✓ Vector store loaded! Contains {doc_count} documents\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test with question: What is diabetes?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response: Diabetes, also known as diabetes mellitus, is a disease in which blood glucose, or blood sugar, levels are too high. Glucose is the body's main source of energy and comes from the food you eat, as wel...\n",
            "\n",
            "Retrieved contexts: 3\n"
          ]
        }
      ],
      "source": [
        "# Create retriever and chain (same pattern as notebook 5)\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Create retriever\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": K_RETRIEVAL})\n",
        "\n",
        "# Initialize LLM\n",
        "llm = init_chat_model(MODEL_NAME, temperature=TEMPERATURE)\n",
        "\n",
        "# Create template and chain\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "qa_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Helper function to format documents\n",
        "def format_docs(relevant_docs):\n",
        "    return \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
        "\n",
        "# Quick test\n",
        "test_question = all_eval_data[0][\"question\"]\n",
        "print(f\"Test with question: {test_question}\")\n",
        "relevant_docs = retriever.invoke(test_question)\n",
        "test_response = qa_chain.invoke({\"context\": format_docs(relevant_docs), \"query\": test_question})\n",
        "print(f\"\\nResponse: {test_response[:200]}...\")\n",
        "print(f\"\\nRetrieved contexts: {len(relevant_docs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 36 questions...\n",
            "  Processing question 5/36...\n",
            "  Processing question 10/36...\n",
            "  Processing question 15/36...\n",
            "  Processing question 20/36...\n",
            "  Processing question 25/36...\n",
            "  Processing question 30/36...\n",
            "  Processing question 35/36...\n",
            "  Processing question 36/36...\n",
            "\n",
            "✓ Dataset created with 36 entries\n"
          ]
        }
      ],
      "source": [
        "# Process all questions and create dataset for RAGAS\n",
        "from ragas import EvaluationDataset\n",
        "\n",
        "print(f\"Processing {len(all_eval_data)} questions...\")\n",
        "dataset = []\n",
        "\n",
        "for i, item in enumerate(all_eval_data, 1):\n",
        "    question = item[\"question\"]\n",
        "    ground_truth = item[\"ground_truth\"]\n",
        "    \n",
        "    if i % 5 == 0 or i == len(all_eval_data):\n",
        "        print(f\"  Processing question {i}/{len(all_eval_data)}...\")\n",
        "    \n",
        "    try:\n",
        "        relevant_docs = retriever.invoke(question)\n",
        "        response = qa_chain.invoke({\"context\": format_docs(relevant_docs), \"query\": question})\n",
        "        dataset.append({\n",
        "            \"user_input\": question,\n",
        "            \"retrieved_contexts\": [rdoc.page_content for rdoc in relevant_docs],\n",
        "            \"response\": response,\n",
        "            \"reference\": ground_truth,\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"  Error processing question {i}: {e}\")\n",
        "        # Add entry with error\n",
        "        dataset.append({\n",
        "            \"user_input\": question,\n",
        "            \"retrieved_contexts\": [],\n",
        "            \"response\": f\"ERROR: {str(e)}\",\n",
        "            \"reference\": ground_truth,\n",
        "        })\n",
        "\n",
        "print(f\"\\n✓ Dataset created with {len(dataset)} entries\")\n",
        "evaluation_dataset = EvaluationDataset.from_list(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing evaluator LLM...\n",
            "\n",
            "Running RAGAS evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_77028/3450162588.py:8: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
            "  evaluator_llm = LangchainLLMWrapper(llm)\n",
            "Evaluating: 100%|██████████| 108/108 [05:12<00:00,  2.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Evaluation completed!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.9051, 'faithfulness': 0.9954, 'factual_correctness(mode=f1)': 0.8386}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run RAGAS evaluation (same pattern as notebook 5)\n",
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
        "from openai import OpenAI\n",
        "\n",
        "print(\"Initializing evaluator LLM...\")\n",
        "evaluator_llm = LangchainLLMWrapper(llm)\n",
        "\n",
        "print(\"\\nRunning RAGAS evaluation...\")\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],\n",
        "    llm=evaluator_llm,\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Evaluation completed!\")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available columns: ['user_input', 'retrieved_contexts', 'response', 'reference', 'context_recall', 'faithfulness', 'factual_correctness(mode=f1)']\n",
            "\n",
            "Metrics by question:\n",
            "================================================================================\n",
            "                                                                    user_input  context_recall  faithfulness  factual_correctness(mode=f1)\n",
            "                                                             What is diabetes?    1.0000000000  1.0000000000                  0.7400000000\n",
            "                                             Why is diabetes a health problem?    1.0000000000  1.0000000000                  0.6200000000\n",
            "                                          What are the main types of diabetes?    1.0000000000  1.0000000000                  0.9100000000\n",
            "                                                  What causes type 1 diabetes?    0.5000000000  1.0000000000                  0.8000000000\n",
            "                                                  What causes type 2 diabetes?    1.0000000000  1.0000000000                  1.0000000000\n",
            "                          Who is at higher risk of developing type 2 diabetes?    1.0000000000  1.0000000000                  1.0000000000\n",
            "                                     What are the common symptoms of diabetes?    1.0000000000  1.0000000000                  0.8400000000\n",
            "                                                    How is diabetes diagnosed?    0.5000000000  1.0000000000                  0.8000000000\n",
            "                                               How is type 1 diabetes treated?    0.6666666667  1.0000000000                  1.0000000000\n",
            "                                               How is type 2 diabetes treated?    0.7500000000  1.0000000000                  0.9100000000\n",
            "                                                    Can diabetes be prevented?    1.0000000000  1.0000000000                  1.0000000000\n",
            "                           What is gestational diabetes and how is it treated?    0.5000000000  1.0000000000                  0.8600000000\n",
            "                                                             What is diabetes?    0.7500000000  1.0000000000                  0.8900000000\n",
            "                                          What are the main types of diabetes?    1.0000000000  1.0000000000                  0.9100000000\n",
            "                                                    How is diabetes diagnosed?    1.0000000000  1.0000000000                  1.0000000000\n",
            "                                  What are the treatments for type 1 diabetes?    1.0000000000  1.0000000000                  0.3300000000\n",
            "                                How can type 2 diabetes be treated or managed?    0.7500000000  0.8333333333                  0.9100000000\n",
            "                                          How is gestational diabetes treated?    1.0000000000  1.0000000000                  0.3300000000\n",
            "                                             Can type 1 diabetes be prevented?    1.0000000000  1.0000000000                  1.0000000000\n",
            "                            How can you help delay or prevent type 2 diabetes?    1.0000000000  1.0000000000                  1.0000000000\n",
            "                      Can lifestyle changes help prevent gestational diabetes?    1.0000000000  1.0000000000                  0.6700000000\n",
            "                                                       What is blood pressure?    1.0000000000  1.0000000000                  1.0000000000\n",
            "                                         How is high blood pressure diagnosed?    0.6666666667  1.0000000000                  0.8000000000\n",
            "                          What are the categories of blood pressure in adults?    0.8333333333  1.0000000000                  0.8300000000\n",
            "                               What are the main types of high blood pressure?    1.0000000000  1.0000000000                  0.4400000000\n",
            "                                         Why is high blood pressure dangerous?    1.0000000000  1.0000000000                  0.8900000000\n",
            "                              What are the treatments for high blood pressure?    1.0000000000  1.0000000000                  0.6700000000\n",
            "                                                  What is high blood pressure?    1.0000000000  1.0000000000                  1.0000000000\n",
            "                    What lifestyle changes can help lower high blood pressure?    1.0000000000  1.0000000000                  1.0000000000\n",
            "          What happens if lifestyle changes alone cannot lower blood pressure?    1.0000000000  1.0000000000                  0.5000000000\n",
            "                  How do ACE inhibitors and ARBs work to lower blood pressure?    1.0000000000  1.0000000000                  1.0000000000\n",
            "                               How do beta blockers help lower blood pressure?    1.0000000000  1.0000000000                  0.8300000000\n",
            "                         How do calcium channel blockers lower blood pressure?    1.0000000000  1.0000000000                  1.0000000000\n",
            "                                        How do diuretics lower blood pressure?    1.0000000000  1.0000000000                  0.9100000000\n",
            "                     Why might you need more than one blood pressure medicine?    1.0000000000  1.0000000000                  1.0000000000\n",
            "If you are taking blood pressure medicines, do lifestyle changes still matter?    0.6666666667  1.0000000000                  0.8000000000\n",
            "\n",
            "\n",
            "Aggregated metrics (average):\n",
            "================================================================================\n",
            "Context Recall                 0.9050925926\n",
            "Faithfulness                   0.9953703704\n",
            "Factual Correctness (F1)       0.8386111111\n"
          ]
        }
      ],
      "source": [
        "# Visualize detailed results\n",
        "import pandas as pd\n",
        "\n",
        "result_df = result.to_pandas()\n",
        "\n",
        "print(\"Available columns:\", result_df.columns.tolist())\n",
        "print(\"\\nMetrics by question:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Show all metric columns with full precision\n",
        "metric_cols = [col for col in result_df.columns if col not in \n",
        "               ['user_input', 'retrieved_contexts', 'response', 'reference']]\n",
        "available_cols = [\"user_input\"] + metric_cols\n",
        "available_cols = [col for col in available_cols if col in result_df.columns]\n",
        "\n",
        "# Create a copy to format for display without modifying original precision\n",
        "display_df = result_df[available_cols].copy()\n",
        "\n",
        "# Format numeric columns to show more decimal places\n",
        "pd.set_option('display.float_format', lambda x: f'{x:.10f}' if pd.notna(x) else 'NaN')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "\n",
        "print(display_df.to_string(index=False))\n",
        "\n",
        "# Reset pandas display options\n",
        "pd.reset_option('display.float_format')\n",
        "pd.reset_option('display.max_columns')\n",
        "pd.reset_option('display.width')\n",
        "pd.reset_option('display.max_colwidth')\n",
        "\n",
        "print(\"\\n\\nAggregated metrics (average):\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Display all available metrics\n",
        "metric_cols = [col for col in result_df.columns if col not in \n",
        "               ['user_input', 'retrieved_contexts', 'response', 'reference']]\n",
        "\n",
        "for metric_col in metric_cols:\n",
        "    if metric_col in result_df.columns:\n",
        "        mean_val = result_df[metric_col].mean()\n",
        "        # Format metric name for display\n",
        "        display_name = metric_col.replace('_', ' ').title()\n",
        "        if metric_col == \"factual_correctness(mode=f1)\":\n",
        "            display_name = \"Factual Correctness (F1)\"\n",
        "        print(f\"{display_name:30s} {mean_val:.10f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Results saved to: ../data/eval_results/ragas_evaluation_results.json\n",
            "✓ Results also saved as CSV: ../data/eval_results/ragas_evaluation_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Save results\n",
        "OUTPUT_DIR = Path(\"../data/eval_results\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save detailed results\n",
        "output_file = OUTPUT_DIR / \"ragas_evaluation_results.json\"\n",
        "\n",
        "# Convert DataFrame to dict preserving full precision\n",
        "# Use float_format=None to preserve full precision in to_dict\n",
        "detailed_results = []\n",
        "for _, row in result_df.iterrows():\n",
        "    record = {}\n",
        "    for col in result_df.columns:\n",
        "        value = row[col]\n",
        "        # Preserve full precision for numeric values\n",
        "        if pd.api.types.is_numeric_dtype(result_df[col]):\n",
        "            record[col] = float(value) if pd.notna(value) else None\n",
        "        else:\n",
        "            record[col] = value\n",
        "    detailed_results.append(record)\n",
        "\n",
        "results_dict = {\n",
        "    \"total_questions\": len(all_eval_data),\n",
        "    \"metrics_summary\": {},\n",
        "    \"detailed_results\": detailed_results,\n",
        "}\n",
        "\n",
        "# Add all available metrics to summary (preserve full precision)\n",
        "metric_cols = [col for col in result_df.columns if col not in \n",
        "               ['user_input', 'retrieved_contexts', 'response', 'reference']]\n",
        "\n",
        "for metric_col in metric_cols:\n",
        "    if metric_col in result_df.columns:\n",
        "        results_dict[\"metrics_summary\"][metric_col] = float(result_df[metric_col].mean())\n",
        "\n",
        "# Use ensure_ascii=False and allow_nan=False to preserve precision\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results_dict, f, indent=2, ensure_ascii=False, allow_nan=False)\n",
        "\n",
        "print(f\"✓ Results saved to: {output_file}\")\n",
        "\n",
        "# Also save as CSV for easy visualization\n",
        "# Use float_format to preserve full precision (None means full precision)\n",
        "csv_file = OUTPUT_DIR / \"ragas_evaluation_results.csv\"\n",
        "result_df.to_csv(csv_file, index=False, float_format='%.15g')\n",
        "print(f\"✓ Results also saved as CSV: {csv_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics by evaluation file:\n",
            "================================================================================\n",
            "\n",
            "qa_ground_truth_diabetes_overview_v2.json:\n",
            "  Questions: 15\n",
            "  Faithfulness:           1.0000000000\n",
            "  Context Recall:         0.8444444444\n",
            "  Factual Correctness:    0.8853333333\n",
            "\n",
            "qa_ground_truth_diabetes_treatment.json:\n",
            "  Questions: 12\n",
            "  Faithfulness:           0.9861111111\n",
            "  Context Recall:         0.9166666667\n",
            "  Factual Correctness:    0.7908333333\n",
            "\n",
            "qa_ground_truth_hypertension_overview.json:\n",
            "  Questions: 6\n",
            "  Faithfulness:           1.0000000000\n",
            "  Context Recall:         0.9166666667\n",
            "  Factual Correctness:    0.7716666667\n",
            "\n",
            "qa_ground_truth_hypertension_treatment.json:\n",
            "  Questions: 9\n",
            "  Faithfulness:           1.0000000000\n",
            "  Context Recall:         0.9629629630\n",
            "  Factual Correctness:    0.8933333333\n"
          ]
        }
      ],
      "source": [
        "# Analysis by evaluation file\n",
        "print(\"Metrics by evaluation file:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "file_metrics = {}\n",
        "for eval_file in eval_files:\n",
        "    file_data = [item for item in all_eval_data if item.get(\"eval_file\") == eval_file.name]\n",
        "    if file_data:\n",
        "        # Find corresponding indices in dataset\n",
        "        file_questions = {item[\"question\"] for item in file_data}\n",
        "        file_indices = [i for i, d in enumerate(dataset) if d[\"user_input\"] in file_questions]\n",
        "        \n",
        "        if file_indices:\n",
        "            file_df = result_df.iloc[file_indices]\n",
        "            metrics = {\n",
        "                \"questions\": len(file_indices),\n",
        "                \"file_name\": eval_file.name\n",
        "            }\n",
        "            \n",
        "            # Collect all available metrics\n",
        "            metric_columns = [col for col in result_df.columns if col not in \n",
        "                            ['user_input', 'retrieved_contexts', 'response', 'reference']]\n",
        "            \n",
        "            for metric_col in metric_columns:\n",
        "                if metric_col in file_df.columns:\n",
        "                    metrics[metric_col] = float(file_df[metric_col].mean())\n",
        "            \n",
        "            file_metrics[eval_file.name] = metrics\n",
        "            \n",
        "            print(f\"\\n{eval_file.name}:\")\n",
        "            print(f\"  Questions: {len(file_indices)}\")\n",
        "            \n",
        "            # Display all metrics\n",
        "            if \"faithfulness\" in file_df.columns:\n",
        "                mean_faith = file_df['faithfulness'].mean()\n",
        "                print(f\"  Faithfulness:           {mean_faith:.10f}\")\n",
        "            if \"context_recall\" in file_df.columns:\n",
        "                mean_recall = file_df['context_recall'].mean()\n",
        "                print(f\"  Context Recall:         {mean_recall:.10f}\")\n",
        "            if \"llm_context_recall\" in file_df.columns:\n",
        "                mean_llm_recall = file_df['llm_context_recall'].mean()\n",
        "                print(f\"  LLM Context Recall:     {mean_llm_recall:.10f}\")\n",
        "            if \"factual_correctness(mode=f1)\" in file_df.columns:\n",
        "                mean_factual = file_df['factual_correctness(mode=f1)'].mean()\n",
        "                print(f\"  Factual Correctness:    {mean_factual:.10f}\")\n",
        "            \n",
        "            # Display any other metrics that might exist\n",
        "            other_metrics = [col for col in metric_columns if col not in \n",
        "                           ['faithfulness', 'context_recall', 'llm_context_recall', 'factual_correctness(mode=f1)']]\n",
        "            for metric_col in other_metrics:\n",
        "                if metric_col in file_df.columns:\n",
        "                    mean_val = file_df[metric_col].mean()\n",
        "                    print(f\"  {metric_col}:           {mean_val:.10f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk Markdown Documents\n",
    "\n",
    "This notebook splits Markdown files into smaller chunks for RAG processing and saves them to `data/chunks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mateusdelai/Desktop/applied-ai-health-rag/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "processed_dir = Path(\"../data/processed/health_education\")\n",
    "chunks_dir = Path(\"../data/chunks\")\n",
    "chunks_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure text splitter\n",
    "# Chunk size: 1000 characters (good balance for RAG)\n",
    "# Chunk overlap: 200 characters (maintains context between chunks)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n## \", \"\\n\\n### \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata_from_path(file_path, processed_dir):\n",
    "    \"\"\"\n",
    "    Extracts metadata from the file path structure.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the markdown file\n",
    "        processed_dir: Base processed directory\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metadata dictionary\n",
    "    \"\"\"\n",
    "    relative_path = file_path.relative_to(processed_dir)\n",
    "    parts = relative_path.parts\n",
    "    \n",
    "    metadata = {\n",
    "        \"source_file\": str(file_path),\n",
    "        \"filename\": file_path.name,\n",
    "        \"file_stem\": file_path.stem\n",
    "    }\n",
    "    \n",
    "    # Extract type (condition/treatment)\n",
    "    if len(parts) > 0:\n",
    "        metadata[\"type\"] = parts[0]\n",
    "    \n",
    "    # Extract condition\n",
    "    if len(parts) > 1:\n",
    "        metadata[\"condition\"] = parts[1]\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_markdown_file(md_path, processed_dir, text_splitter):\n",
    "    \"\"\"\n",
    "    Chunks a Markdown file and returns a list of chunk dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        md_path: Path to the Markdown file\n",
    "        processed_dir: Base processed directory\n",
    "        text_splitter: Text splitter instance\n",
    "    \n",
    "    Returns:\n",
    "        list: List of chunk dictionaries\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing: {md_path}\")\n",
    "        \n",
    "        # Read markdown content\n",
    "        with open(md_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = extract_metadata_from_path(md_path, processed_dir)\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = text_splitter.split_text(content)\n",
    "        \n",
    "        # Create chunk dictionaries with metadata\n",
    "        chunk_list = []\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            chunk_dict = {\n",
    "                \"chunk_id\": f\"{metadata['file_stem']}_chunk_{i:04d}\",\n",
    "                \"chunk_index\": i,\n",
    "                \"text\": chunk_text,\n",
    "                \"metadata\": metadata.copy()\n",
    "            }\n",
    "            chunk_list.append(chunk_dict)\n",
    "        \n",
    "        print(f\"  ✓ Created {len(chunk_list)} chunks\")\n",
    "        print(f\"  Total characters: {sum(len(c['text']) for c in chunk_list):,}\\n\")\n",
    "        \n",
    "        return chunk_list\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error processing {md_path}: {e}\\n\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 Markdown files to chunk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all Markdown files in the processed directory\n",
    "md_files = list(processed_dir.rglob(\"*.md\"))\n",
    "print(f\"Found {len(md_files)} Markdown files to chunk\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../data/processed/health_education/conditions/hypertension/hypertension_medlineplus_overview.md\n",
      "  ✓ Created 41 chunks\n",
      "  Total characters: 26,393\n",
      "\n",
      "Processing: ../data/processed/health_education/conditions/diabetes/diabetes_medlineplus_overview.md\n",
      "  ✓ Created 46 chunks\n",
      "  Total characters: 28,944\n",
      "\n",
      "Processing: ../data/processed/health_education/treatments/hypertension/hypertension_medlineplus_treatment.md\n",
      "  ✓ Created 21 chunks\n",
      "  Total characters: 13,058\n",
      "\n",
      "Processing: ../data/processed/health_education/treatments/diabetes/diabetes_medlineplus_treatment.md\n",
      "  ✓ Created 23 chunks\n",
      "  Total characters: 14,037\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all Markdown files and create chunks\n",
    "all_chunks = []\n",
    "results = []\n",
    "\n",
    "for md_path in md_files:\n",
    "    chunks = chunk_markdown_file(md_path, processed_dir, text_splitter)\n",
    "    \n",
    "    if chunks:\n",
    "        all_chunks.extend(chunks)\n",
    "        results.append({\n",
    "            \"file\": str(md_path),\n",
    "            \"chunks_count\": len(chunks),\n",
    "            \"success\": True\n",
    "        })\n",
    "    else:\n",
    "        results.append({\n",
    "            \"file\": str(md_path),\n",
    "            \"chunks_count\": 0,\n",
    "            \"success\": False\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved 131 chunks to: ../data/chunks/all_chunks.json\n",
      "  File size: 138,453 bytes\n"
     ]
    }
   ],
   "source": [
    "# Save all chunks to a single JSON file\n",
    "chunks_file = chunks_dir / \"all_chunks.json\"\n",
    "\n",
    "with open(chunks_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✓ Saved {len(all_chunks)} chunks to: {chunks_file}\")\n",
    "print(f\"  File size: {chunks_file.stat().st_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved chunks grouped by file to: ../data/chunks/chunks_by_file.json\n",
      "  Files processed: 4\n"
     ]
    }
   ],
   "source": [
    "# Also save chunks grouped by file for easier access\n",
    "chunks_by_file = {}\n",
    "for chunk in all_chunks:\n",
    "    source_file = chunk['metadata']['source_file']\n",
    "    if source_file not in chunks_by_file:\n",
    "        chunks_by_file[source_file] = []\n",
    "    chunks_by_file[source_file].append(chunk)\n",
    "\n",
    "chunks_by_file_path = chunks_dir / \"chunks_by_file.json\"\n",
    "with open(chunks_by_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks_by_file, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✓ Saved chunks grouped by file to: {chunks_by_file_path}\")\n",
    "print(f\"  Files processed: {len(chunks_by_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CHUNKING SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total files: 4\n",
      "Successful: 4\n",
      "Failed: 0\n",
      "Total chunks created: 131\n",
      "\n",
      "------------------------------------------------------------\n",
      "Files processed:\n",
      "------------------------------------------------------------\n",
      "✓ hypertension_medlineplus_overview.md: 41 chunks\n",
      "✓ diabetes_medlineplus_overview.md: 46 chunks\n",
      "✓ hypertension_medlineplus_treatment.md: 21 chunks\n",
      "✓ diabetes_medlineplus_treatment.md: 23 chunks\n"
     ]
    }
   ],
   "source": [
    "# Summary of chunking process\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHUNKING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "successful = sum(1 for r in results if r[\"success\"])\n",
    "total = len(results)\n",
    "total_chunks = sum(r[\"chunks_count\"] for r in results)\n",
    "\n",
    "print(f\"\\nTotal files: {total}\")\n",
    "print(f\"Successful: {successful}\")\n",
    "print(f\"Failed: {total - successful}\")\n",
    "print(f\"Total chunks created: {total_chunks}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Files processed:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for result in results:\n",
    "    status = \"✓\" if result[\"success\"] else \"✗\"\n",
    "    filename = Path(result[\"file\"]).name\n",
    "    print(f\"{status} {filename}: {result['chunks_count']} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAMPLE CHUNK\n",
      "============================================================\n",
      "\n",
      "Chunk ID: hypertension_medlineplus_overview_chunk_0000\n",
      "Source: hypertension_medlineplus_overview.md\n",
      "Type: conditions\n",
      "Condition: hypertension\n",
      "\n",
      "Text preview (first 200 chars):\n",
      "------------------------------------------------------------\n",
      "# High Blood Pressure\n",
      "\n",
      "Also called: Benign essential hypertension, Essential hypertension, HBP, HTN, Hypertension\n",
      "\n",
      "On this page\n",
      "\n",
      "### Basics\n",
      "\n",
      "- [Summary](#summary)\n",
      "- [Start Here](#cat_51)\n",
      "- [Symptoms](...\n",
      "\n",
      "Full chunk length: 815 characters\n"
     ]
    }
   ],
   "source": [
    "# Display sample chunk for inspection\n",
    "if all_chunks:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAMPLE CHUNK\")\n",
    "    print(\"=\" * 60)\n",
    "    sample = all_chunks[0]\n",
    "    print(f\"\\nChunk ID: {sample['chunk_id']}\")\n",
    "    print(f\"Source: {sample['metadata']['filename']}\")\n",
    "    print(f\"Type: {sample['metadata'].get('type', 'N/A')}\")\n",
    "    print(f\"Condition: {sample['metadata'].get('condition', 'N/A')}\")\n",
    "    print(f\"\\nText preview (first 200 chars):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(sample['text'][:200] + \"...\" if len(sample['text']) > 200 else sample['text'])\n",
    "    print(f\"\\nFull chunk length: {len(sample['text'])} characters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
